# deletes previous variables
rm(list=ls())
# reads wavelet data from file .csv
data <- read.csv("locationfile.csv", header = FALSE)

# we check that the empty ones take a zero
data[is.na(data)] <- 0

# we put together a data framework
data <- data.frame(sapply(data, function(x) as.numeric(as.character(x))))
data[is.na(data)] <- 0

# view the first data
head(data)
str(data)

# assigns names to classes
names(data) <- c('clase','v1','v2','v3','v4','v5','v6','v7','v8','v9','v10','v11','v12','v13','v14','v15','v16','v17','v18','v19','v20','v21','v22','v23','v24') # coloca el nombre como columna
colSums(is.na(data))

# sample summary
summary(data)

# to each dataset assign a class to each dataset
pairs(data[2:23], col=data$clase)

# class coding is carried out (0 1)
library(dummies)
data_dm <- dummy.data.frame(data=data, names="clase", sep="_")
str(data_dm)

# normalises the data
normaliza <- function(x) {return ((x-min(x))/(max(x)-min(x)))}
data_norm <- as.data.frame(lapply(data_dm, normaliza))

# sample summary data
summary(data_norm)


set.seed(3141592) #necessary if the same code is to be reproduced again and the same results are to be obtained.

# divides the dataset into train and test
index <- sample(nrow(data_norm), round(0.75*nrow(data_norm))) # randomly sampled
train <- data_norm[index,] # creates train from the sample index
test <- data_norm[-index,] # creates test from the rest of the sample
head(train)


# if there are cells with no value it assigns them 0
train[is.na(train)] <- 0
test[is.na(test)] <- 0


# neural network model
set.seed(3141592)
library(neuralnet)
ann_model <- neuralnet(clase_1+clase_2+clase_3+clase_4~., data=train, hidden=c(20, 10, 10), linear.output = FALSE, rep =2000) #100 80 30 30rep### 100 80 40 20rep

plot(ann_model, rep="best")


# computes test data
ann_pred <- compute(ann_model,test[,5:28])
head(ann_pred$net.result)

# round up to find out which class he predicted
ann_pred_round <- as.data.frame(round(ann_pred$net.result))
head(ann_pred_round, 10)

predic<-max.col(ann_pred_round)
head(predic, 100)

test_res <- max.col(test[,1:4])
head(test_res, 100)


# library is called in order to be able to use the function
library(nnet)
library(gmodels)
library(caret)

CrossTable(x=test_res, y=predic)

# confusion matrix
u <- union(predic, test_res)
t <- table(factor(predic, u), factor(test_res, u))
confusionMatrix(t)


#Cross validation

set.seed(3141592)
myfolds <- createFolds(data$clase, k=10) # how many times you want the training and test subsets from the original database

CV_ANN <- function(x)
{
  train <- data_norm[-x,]
  test <- data_norm[x,]
  ann_model <- neuralnet(clase_1+clase_2+clase_3+clase_4~., data=train, hidden=c(20, 10, 10), linear.output = FALSE, rep =1000, learningrate.factor = list(minus = 0.001, plus = 100))
  
  ann_pred <- compute(ann_model,test[,5:28])
  ann_pred_round <- as.data.frame(round(ann_pred$net.result))
  predic <- max.col(ann_pred_round)
  test_res <- max.col(test[,1:4])
  u <- union(predic, test_res)
  t <- table(factor(predic, u), factor(test_res, u))
  myconfusion <-confusionMatrix(t)
  
  return (myconfusion$overall[1:2]) # the range [1:2] corresponds to the indexes where the Accuracy and Kappa variables are located in our confusion matrix.
}

CV_result <- lapply(myfolds, CV_ANN)
CV_result_DF <-as.data.frame(CV_result)
CV_result_DF

rowMeans(CV_result_DF)
plot(ann_model, rep="best")
